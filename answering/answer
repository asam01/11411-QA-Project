#!/usr/bin/env python3

import spacy
import nltk
import benepar
from benepar.spacy_plugin import BeneparComponent
import reverseAsk
import answerBinary
import find_sentence
import sys 
import io 
import itertools


nlp = spacy.load("en_core_web_sm")
nlp.add_pipe(BeneparComponent('benepar_en2'))
nltk.download('averaged_perceptron_tagger')
    
# uncomment to suppress warnings about version
import warnings
warnings.filterwarnings("ignore")
#add case for how many questions 
def answerMain(question, corpus):
    qnWord = question.split(" ")[0] # NOTE question is not necessarily the first word
    qnWords = question.split(" ")[:2] 
    sentenceList = find_sentence.find_sentence(question, corpus)
    if qnWord.lower() in ["who", "where", "which", "when", "what"]:
        answer = reverseAsk.answerWh(question, sentenceList)
  
    else: 
        answer = [answerBinary.answerBinary(question, sentenceList)]
    return answer

if __name__ == '__main__':
    if len(sys.argv) != 3:
        print("Usage: python3 answer.py article.txt questions.txt")
        sys.exit(1)

    article = sys.argv[1]
    questions = (sys.argv[2])
    
    article_string = ""
    questions_string = "" 
    with io.open(article, 'r', encoding='utf8') as f:
        article_string = f.read()
    with io.open(questions, 'r', encoding='utf8') as f: 
        questions_string = f.read() 
    qs = questions_string.split("\n")

    answers = [] 
    for q in qs:
        answers.append(answerMain(q, article_string)) 

    
    #all_answers = list(itertools.chain(*answers))
    # output to stdout  
    for a in answers: 
        print(a[0])




